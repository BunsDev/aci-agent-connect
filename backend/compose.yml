services:
  db:
    # Note: we need to use the pgvector/pgvector:pg17 image to use the pgvector extension, instead of the official postgres image
    image: pgvector/pgvector:pg17
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d local_db"]
      interval: 1s
      retries: 5
      start_period: 1s
      timeout: 10s
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: local_db
    ports:
      - "5432:5432"
    restart: no

  test-db:
    image: pgvector/pgvector:pg17
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d local_db"]
      interval: 1s
      retries: 5
      start_period: 1s
      timeout: 10s
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: local_db
    restart: no
  opensearch:
    image: opensearchproject/opensearch:2.11.1
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - "DISABLE_SECURITY_PLUGIN=true"
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
      - "network.host=0.0.0.0"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data:/usr/share/opensearch/data
    ports:
      - "9200:9200"
      - "9600:9600"
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q 'status.*green\\|yellow'"]
      interval: 20s
      timeout: 20s
      retries: 10
      start_period: 60s

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.11.1
    environment:
      - 'OPENSEARCH_HOSTS=["http://opensearch:9200"]'
      - "DISABLE_SECURITY_DASHBOARDS_PLUGIN=true"
    ports:
      - "5601:5601"
    depends_on:
      opensearch:
        condition: service_healthy

  aws:
    image: localstack/localstack
    ports:
      - "4566:4566"
    environment:
      - DEBUG=1
      - DATA_DIR=/var/lib/localstack/data
      - SERVICES=kms
      - LS_LOG=warn
    volumes:
      - ./scripts/create-kms-encryption-key.sh:/etc/localstack/init/ready.d/create-kms-encryption-key.sh

  propelauth_mock:
    build:
      context: .
      dockerfile: Dockerfile.runner
    ports:
      - "12800:12800"
    working_dir: /workdir
    volumes:
      - ./mock/propelauth_mock_server.py:/workdir/propelauth_mock_server.py
    command: uvicorn propelauth_mock_server:app --proxy-headers --forwarded-allow-ips=* --host 0.0.0.0 --port 12800 --reload

  server:
    build:
      context: .
      dockerfile: Dockerfile.server
    healthcheck:
      test: ["CMD-SHELL", "curl localhost:8000/v1/health"]
      interval: 10s
      retries: 5
      start_period: 3s
      timeout: 10s
    env_file:
      - .env.local
    # useful for development, changes take effect in the container without needing to rebuild the image.
    volumes:
      - ./aci/server:/workdir/aci/server
      - ./aci/common:/workdir/aci/common
      # Mocks out the propelauth_fastapi module in server container to bypass token validation in local development
      - ./mock/propelauth_fastapi_mock.py:/workdir/.venv/lib/python3.12/site-packages/propelauth_fastapi/__init__.py
    ports:
      - "8000:8000"
    # this overrides the default CMD specified in the Dockerfile, use this for local development
    command: uvicorn aci.server.main:app --proxy-headers --forwarded-allow-ips=* --host 0.0.0.0 --port 8000 --reload --no-access-log
    depends_on:
      db:
        condition: service_healthy
      aws:
        condition: service_healthy
      opensearch:
        condition: service_healthy
      opensearch-dashboards:
        condition: service_started
    restart: no

  # can think of runner as an staging host for executing any commands
  # e.g., run cli commands and scripts such as seed db etc.
  runner:
    build:
      context: .
      dockerfile: Dockerfile.runner
    env_file:
      - .env.local
    # Mount the local code into the container so you can run scripts / Alembic
    volumes:
      - ./aci:/workdir/aci
      - ./apps:/workdir/apps
      - ./scripts:/workdir/scripts
      - ./evals:/workdir/evals
      - ./alembic.ini:/workdir/alembic.ini
      # Mocks out the propelauth_fastapi module in server container to bypass token validation in local development
      - ./mock/propelauth_fastapi_mock.py:/workdir/.venv/lib/python3.12/site-packages/propelauth_fastapi/__init__.py
    command: >
      /bin/sh -c "alembic upgrade head && tail -f /dev/null"
    depends_on:
      db:
        condition: service_healthy
      server:
        condition: service_healthy

  test-runner:
    build:
      context: .
      dockerfile: Dockerfile.runner
    env_file:
      - .env.local
    environment:
      # override the db host to use the test-db service
      - SERVER_DB_HOST=test-db
      - ALEMBIC_DB_HOST=test-db
      - CLI_DB_HOST=test-db
    # Mount the local code into the container so you can run scripts / Alembic
    volumes:
      - ./aci:/workdir/aci
      - ./apps:/workdir/apps
      - ./scripts:/workdir/scripts
      - ./evals:/workdir/evals
      - ./alembic.ini:/workdir/alembic.ini
      # Mocks out the propelauth_fastapi module in server container to bypass token validation in local development
      - ./mock/propelauth_fastapi_mock.py:/workdir/.venv/lib/python3.12/site-packages/propelauth_fastapi/__init__.py
    command: >
      /bin/sh -c "alembic upgrade head && tail -f /dev/null"
    depends_on:
      test-db:
        condition: service_healthy
      aws:
        condition: service_healthy
volumes:
  opensearch-data:
